---
title: "R Notebook"
output: html_notebook
---

```{r}
library(glmnet)
library(e1071)
train <- read.csv("train_set.csv")
val <- read.csv("validation_set.csv")
test <- read.csv("test_set.csv")
use <- c('Class','SMA5','SMA15','SMA20','SMA200',
        'EMA10Cross','EMA20Cross','EMA26Cross','EMA50Cross','EMA100Cross','EMA200Cross',
        'MACD','Volume','Price',
        'Up.Down10','Up.Down15','Up.Down50','Up.Down100',
        'SMA5Cross','SMA10Cross','SMA15Cross','SMA20Cross','SMA50Cross','SMA100Cross','SMA200Cross') 
train <- train[,(names(train) %in% use)]
val <- val[,(names(val) %in% use)]
test <- test[,(names(test) %in% use)]
```

as.factor
```{r}
col_names = c('Class', 'EMA10Cross','EMA20Cross','EMA26Cross','EMA50Cross','EMA100Cross','EMA200Cross', 'SMA5Cross','SMA10Cross','SMA15Cross','SMA20Cross','SMA50Cross','SMA100Cross','SMA200Cross') 
train[col_names] <- lapply(train[col_names] , factor)
val[col_names] <- lapply(val[col_names] , factor)
test[col_names] <- lapply(test[col_names] , factor)
```

Logistic Implementation and Regularization
```{r}
logistic1 <- glm(Class ~. , train, family="binomial")
summary(logistic1)

mat <- model.matrix(Class ~ .,data=train)[,-1]
reg_lasso <- cv.glmnet(mat,train$Class,alpha = 1, family="binomial")
reg_ridge <- cv.glmnet(mat,train$Class,alpha = 0, family="binomial")
reg_elasticnet <- cv.glmnet(mat,train$Class,alpha = 0.5, family="binomial")
model_lasso <- glmnet(mat,train$Class,alpha = 1, family="binomial")
model_ridge <- glmnet(mat,train$Class,alpha = 0, family="binomial")
model_elasticnet <- glmnet(mat,train$Class,alpha = 0.5, family="binomial")

coef(reg_lasso, s=reg_lasso$lambda.min)
coef(reg_ridge, s=reg_ridge$lambda.min)
coef(reg_elasticnet, s=reg_elasticnet$lambda.min)
```

SVM Implementation
```{r}
set.seed(1)
#Tuning to select best parameters to use
tune_linear <- tune(svm, Class~., data=train, kernel="linear", ranges=list(cost=seq(0.1,2,0.1)))
summary(tune_linear)
tune_poly <- tune(svm, Class~., data=train, kernel="polynomial", ranges=list(cost=seq(0.1,2,0.1)))
summary(tune_poly)
```

```{r}
set.seed(1)
complex_kernel_train <- sample(seq_len(nrow(train)), size = 0.5*nrow(train))
new_train <- train[complex_kernel_train, ]
tune_sigmoid2 <- tune(svm, Class~., data=new_train, kernel="sigmoid", ranges=list(cost=seq(0.1,2,0.1)))
summary(tune_sigmoid2)
```



```{r}
set.seed(1)
tune_radial <- tune(svm, Class~., data=new_train, kernel="radial", ranges=list(cost=seq(0.1,2,0.1)))
summary(tune_radial)
```

```{r}
#Fitting Models
svc <- svm(Class~., data=train, kernel="linear", cost=1.1)
svm_poly <- svm(Class~., data=train, kernel="polynomial", cost=0.8)
```

```{r}
svm_radial <- svm(Class~., data=train, kernel="radial", cost=0.6)
svm_sigmoid <- svm(Class~., data=train, kernel="sigmoid", cost=0.2)
```

```{r}
#Val Set Performance
(svc_val_miss <- mean(val$Class!=predict(svc, newdata=val)))
(svm_p_val_miss <- mean(val$Class!=predict(svm_poly, newdata=val)))
(svm_r_val_miss <- mean(val$Class!=predict(svm_radial, newdata=val)))
(svm_s_val_miss <- mean(val$Class!=predict(svm_radial, newdata=val)))

#Test Set Prediction
prediction <- predict(svm_poly, newdata = test)
export_svm <- data.frame(Prediction = prediction)
write.csv(export_svm, file="svm__polynomial_prediciton.csv")
```